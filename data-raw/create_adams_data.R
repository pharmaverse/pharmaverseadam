# Save specs as JSON file for traceability of changes ----

# Load required library
library(readxl)
library(jsonlite)
library(cli)
library(stringr)
library(dplyr)

# Fix PATH to include Rscript (platform-aware)
path_sep <- if (.Platform$OS.type == "windows") ";" else ":"
Sys.setenv(PATH = paste(R.home("bin"), Sys.getenv("PATH"), sep = path_sep))

# Load metadata
json_file <- "inst/extdata/adams-specs.json"
excel_file <- "inst/extdata/adams-specs.xlsx"

specs_xlsx <- read_excel(excel_file)
specs_json <- toJSON(specs_xlsx, pretty = TRUE)

sheet_names <- excel_sheets(excel_file)

all_sheets <- lapply(sheet_names, function(sheet) {
  read_excel(excel_file, sheet = sheet)
})

names(all_sheets) <- sheet_names
json_data <- toJSON(all_sheets, pretty = TRUE)

write(json_data, file = json_file)

# Ensure all packages are installed ----
update_pkg <- TRUE

# Exclude some templates from {pharmaverseadam} ----
ignore_templates <- list(
  "admiralonco" = c("ad_adrs_basic.R"),
  "admiralmetabolic" = c("ad_adsl_metabolic.R")
)

# Functions to load and save .rda files ----
save_rda <- function(data, file_path, new_name) {
  if (missing(new_name)) {
    save(data, file = file_path, compress = "bzip2")
  } else {
    assign(new_name, data)
    save(list = new_name, file = file_path, compress = "bzip2")
  }
}

load_rda <- function(fileName) {
  load(fileName)
  get(ls()[ls() != "fileName"])
}

# Helper functions ----
# Helper function to retrieve the label attribute for a column
#' @description Retrieve column label attribute or return a default.
#' @param data The dataset containing the column.
#' @param col_name The name of the column.
#' @return A string containing the label attribute or "undocumented field".
get_attr <- function(data, col_name) {
  att <- attr(data[[col_name]], "label")
  if (is.null(att)) {
    att <- "undocumented field"
  } else if (att == "null") {
    att <- "undocumented field"
  }
  return(att)
}

# Helper function to retrieve the dataset keyword to group by TA
#' @description Retrieve the dataset keyword from suffix to group datasets by TA.
#' @param dataset_name The name of the dataset.
get_dataset_keyword <- function(dataset_name, suffix) {
  print(dataset_name)
  print(suffix)
  if (suffix != "") {
    dataset_name <- gsub(suffix, suffixes_dict[suffix], dataset_name)
  }
  dataset_name <- toupper(dataset_name)

  if (is.null(dataset_name) || !nzchar(as.character(dataset_name))) {
    return("generic")
  }
  ds <- as.character(dataset_name)

  # Map suffix codes (used in dataset names from the specs) to TA keyword
  # These are the suffix codes used in this script: _P, _O, _V, _E, _M
  # Update as necessary while adding Extension Packages
  keyword_map <- c(
    "_P" = "ophthalmology",
    "_O" = "oncology",
    "_V" = "vaccine",
    "_E" = "pediatrics",
    "_M" = "metabolic"
  )

  # Check for a matching suffix (case-insensitive)
  for (s in names(keyword_map)) {
    pattern <- paste0(s, "$")
    if (str_detect(ds, regex(pattern, ignore_case = TRUE))) {
      return(keyword_map[[s]])
    }
  }

  # Fallback for datasets without a TA suffix, e.g. from admiral
  return("generic")
}

# Create documentation ----
# Main function to write documentation for a dataset
#' @description Generate R documentation for a dataset.
#' @param data The dataset object.
#' @param dataset_name The name of the dataset.
#' @param dataset_label The label for the dataset. As per all_sheets$Datasets$Label
#' @param pkg The package or extension on which the dataset depends
#' @param template_name The template name from package to run
#' @param dataset_paramnames Parameter names and parameter codes available in the dataset. Defaults to NULL if not available.
#' @param dataset_keyword A description of Therapeutic Area
write_doc <- function(data,
                      dataset_name,
                      dataset_label,
                      pkg,
                      template_name,
                      dataset_paramnames = NULL,
                      dataset_keyword = "generic") {
  # Create documentation for the current dataset
  # TODO: use metatools/metacore for doc  ?
  dataset_label <- str_replace(dataset_label, "Hys Law", "Hy's Law")

  doc_string <- paste(
    "# This file is automatically generated by data-raw/create_adams_data.R.",
    "# For updating it please edit inst/extdata/adams-specs.xlsx and rerun create_adams_data.R.",
    "# Manual edits are not recommended, as changes may be overwritten.",
    "#'",
    sprintf("#' @name %s", dataset_name),
    sprintf("#' @title %s", dataset_label),
    sprintf("#' @keywords dataset %s", dataset_keyword),
    "#' @docType data",
    sprintf("#' @format A data frame with %s columns:", ncol(data)),
    "#'   \\describe{",
    paste(sapply(names(data), function(col_name) {
      paste(sprintf("#'     \\item{ %s }{%s}", col_name, get_attr(data, col_name)))
    }, USE.NAMES = FALSE), collapse = "\n"),
    "#'   }",
    sep = "\n"
  )

  # Add PARAM and PARAMCD in details section if available
  if (!is.null(dataset_paramnames) && dataset_paramnames != "") {
    doc_string <- paste(doc_string, sprintf("#' @details %s", dataset_paramnames), sep = "\n")
  }

  # Append source, references and examples after Details section
  doc_string <- paste(
    doc_string,
    "#'", sprintf("#' @source Generated from %s package (template %s).", pkg, template_name),
    "#' @references None",
    "#'", sprintf("#' @examples\n#' data(\"%s\")", dataset_name),
    sep = "\n",
    sprintf("\"%s\"", dataset_name)
  )

  writeLines(doc_string, con = file.path("R", paste0(dataset_name, ".R")))
}

write_labels <- function(data, dataset_name, suffix) {
  print(dataset_name)
  print(suffix)
  if (suffix != "") {
    dataset_name <- gsub(suffix, suffixes_dict[suffix], dataset_name)
  }
  dataset_name <- toupper(dataset_name)
  tryCatch(
    {
      spec <- metacore::select_dataset(mc, dataset_name)
      data <- metatools::set_variable_labels(data, spec)
      data <- xportr::xportr_df_label(data, spec, domain = dataset_name)
    },
    error = function(e) {
      cli_warn(sprintf(
        "Error retrieving dataset %s specs - Please check adams-specs.xlsx file",
        dataset_name
      ))
    }
  )
  return(data)
}

order_data <- function(data, dataset_name, suffix) {
  print(dataset_name)
  print(suffix)
  if (suffix != "") {
    dataset_name <- gsub(suffix, suffixes_dict[suffix], dataset_name)
  }
  dataset_name <- toupper(dataset_name)
  tryCatch(
    {
      spec <- metacore::select_dataset(mc, dataset_name)
      data <- metatools::order_cols(data, spec)
    },
    error = function(e) {
      cli_warn(sprintf(
        "Error retrieving dataset %s specs - Please check adams-specs.xlsx file",
        dataset_name
      ))
    }
  )
  return(data)
}

run_template <- function(tp) {
  if (!tp %in% ignore_templates_pkg) {
    print(sprintf("Running template %s", tp))
    # Run template
    cmd <- c("Rscript", file.path(templates_path, tp))
    system_result <- system2(cmd, stdout = TRUE, stderr = TRUE)
    exit_code <- attr(system_result, "status")
    tp_basename <- basename(tp)

    if (is.null(exit_code)) {
      dataset_dir <- tools::R_user_dir(sprintf("%s_templates_data", pkg), which = "cache")
      rda_file <- gsub(".R", ".rda", tp_basename)
      rda_file <- gsub("ad_", "", rda_file)
      data <- load_rda(file.path(dataset_dir, rda_file))

      print(sprintf("Processing %s file - move it to
             pharmaverse and generate the doc", rda_file))
      suffix <- ""
      if (pkg != "admiral") {
        suffix <- sprintf("_%s", gsub("admiral", "", pkg))
        dataset_keyword <- "generic"
      }
      # Add suffix in case of != admiral
      filename <- gsub("\\.rda$", sprintf("%s.rda", suffix), rda_file)
      output_adam_path <- file.path("data", filename)
      dataset_name <- gsub("\\.rda$", "", filename)

      # Add PARAM and PARAMCD details
      # Check if data contains PARAMCD/PARAM variables
      param_col <- names(data)[str_detect(string = names(data), pattern = "PARAM$")]
      paramcd_col <- names(data)[str_detect(string = names(data), pattern = "PARAMCD")]

      if (length(param_col) == 1 && length(paramcd_col) == 1) {
        # Check both columns exist
        unique_params <- unique(data[c(paramcd_col, param_col)])
        unique_params <- unique_params[order(unique_params[[paramcd_col]]), ]

        tabular <- function(df, ...) {
          if (!is.data.frame(df)) {
            cli_abort(
              "{.val {deparse(substitute(df))}} must be a dataframe, but is {.cls {class(df)}}"
            )
          }

          align <- function(x) if (is.numeric(x)) "r" else "l"
          col_align <- vapply(df, align, character(1))

          cols <- lapply(df, format, ...)
          contents <- do.call(
            "paste",
            c(cols, list(sep = " \\tab ", collapse = "\\cr\n#'   "))
          )

          paste(sprintf("Contains a set of %d unique Parameter Code%s and Parameter%s: ", nrow(unique_params), if_else(nrow(unique_params) == 1, "", "s"), if_else(nrow(unique_params) == 1, "", "s")),
            "\\tabular{", paste(col_align, collapse = ""), "}{\n#'   ",
            paste0("\\strong{", names(df), "}", sep = "", collapse = " \\tab "), " \\cr\n#'   ",
            trimws(contents), "\n#' }\n",
            sep = ""
          )
        }

        paramnames <- tabular(unique_params)
      } else {
        paramnames <- NULL
      }

      # Derive dataset keyword (therapeutic area) from the dataset name suffix
      dataset_keyword <- get_dataset_keyword(dataset_name, suffix)

      # Write labels
      data <- write_labels(data, dataset_name, suffix)

      # Order columns as per specs
      data <- order_data(data, dataset_name, suffix)

      # Save it to pharmaverseadam data package
      save_rda(data, file_path = output_adam_path, new_name = dataset_name)

      # Write doc
      dataset_label <- attributes(data)$label
      if (is.null(paramnames)) {
        dataset_paramnames <- NULL
      } else {
        dataset_paramnames <- if (!is.null(paramnames) && paramnames != "") paramnames else NULL
      }
      write_doc(data, dataset_name, dataset_label, pkg, tp_basename, dataset_paramnames, dataset_keyword)
    }

    # Return output cmd from templates
    return(list(
      pkg = pkg, template = tp, exit_code = exit_code,
      output = paste(system_result, collapse = "\n")
    ))
  }
}

# Wrapper Function for Error Handling
safe_run_template <- function(tp) {
  tryCatch(
    run_template(tp),
    error = function(e) {
      list(
        pkg = pkg,
        template = tp,
        exit_code = 1,
        output = paste(
          "ERROR in template:", tp, "\n",
          "Package:", pkg, "\n",
          "Message:", e$message, "\n",
          "Call:", paste(capture.output(traceback()), collapse = "\n")
        )
      )
    }
  )
}

if (update_pkg) {
  github_pat <- Sys.getenv("GITHUB_TOKEN") # in case of run through github workflows
  # Install pharmaversesdtm dep: TODO: see if we install from github or latest release?
  remotes::install_github(
    "pharmaverse/pharmaversesdtm",
    ref = "main",
    force = TRUE
  )
}

# Dict to match admiral xlsx specs suffixes
suffixes_dict <- list("_ophtha" = "_P", "_onco" = "_O", "_vaccine" = "_V", "_peds" = "_E", "_metabolic" = "_M")
mc <- metacore::spec_to_metacore("inst/extdata/adams-specs.xlsx",
  where_sep_sheet = FALSE,
  quiet = TRUE
)

packages_list <- c("admiral", "admiralonco", "admiralophtha", "admiralvaccine", "admiralpeds", "admiralmetabolic")

all_results <- c()
for (pkg in packages_list) {
  ignore_templates_pkg <- ignore_templates[pkg]
  # Get latest version of the package (current templates from main branch)
  if (update_pkg) {
    # TODO: replace by main once done
    remotes::install_github(sprintf("pharmaverse/%s", pkg),
      ref = "main", auth_token = if (github_pat == "") {
        NULL
      } else {
        github_pat
      },
      upgrade = "always", force = TRUE
    )
  }
  # Get templates scripts
  templates_path <- file.path(system.file(package = pkg), "templates")
  templates <- list.files(templates_path)
  # Copy paste pkg/data folder content to pharmaverseadam/data (some templates have dependency with their internal data,
  # for example admiral templates have all dependencies with admiral_adsl dataset)
  data_path <- file.path(system.file(package = pkg), "data")
  data_files <- list.files(data_path)

  print(sprintf("Processing templates from %s package", pkg))

  # Run templates in parallel (cross-platform approach)
  if (.Platform$OS.type == "windows") {
    cli_alert_info("Windows detected: Processing {length(templates)} template{?s} sequentially")
    results <- lapply(templates, safe_run_template)
  } else {
    # Unix/Linux/macOS: use parallel processing with forking
    num_cores <- min(parallel::detectCores() - 1, length(templates))
    cli_alert_info("Processing {length(templates)} template{?s} in parallel using {num_cores} core{?s}")
    results <- parallel::mclapply(templates, safe_run_template, mc.cores = num_cores)
  }

  all_results <- c(all_results, results)
}

# Print versions of all relevant packages ----
versions_msg <- "\n==== Package Versions Used ====\n"
for (pkg in packages_list) {
  versions_msg <- paste0(versions_msg, sprintf("%s: %s\n", pkg, as.character(utils::packageVersion(pkg))))
}

# Add pharmaversesdtm version to the list
versions_msg <- paste0(versions_msg, sprintf("pharmaversesdtm: %s\n", as.character(utils::packageVersion("pharmaversesdtm"))))
versions_msg <- paste0(versions_msg, "==============================\n\n")
cat(versions_msg)

# Display error message when a template fails ----
cli_div(theme = list(".error-detail" = list(color = "red")))
for (res in all_results) {
  if (!is.null(res$exit_code) && res$exit_code != 0) {
    cli_alert_danger("template {.val {res$template}} failed - package {.pkg {res$pkg}}")
    cli_alert_danger("Error details:")
    error_lines <- strsplit(res$output, "\n")[[1]]
    for (line in error_lines) {
      cli_text("{.error-detail {line}}")
    }
    cli_text("")
  }
}
cli_end()

# Generate the documentation ----
roxygen2::roxygenize(".", roclets = c("rd", "collate", "namespace"))
